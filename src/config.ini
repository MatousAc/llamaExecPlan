[general]
quiet: False
ignoreWarnings: True

[paths]
# paths are relative to the llamaExecPlan dir
baseModel: /models/llama-hf/7b-chat
data: /data/clean
log: /data/logs

[dataFormatter]
delim: ###
# we MUST begin with <s> and a space for the tokenizer to match the
# tokens for inputTemplate below with the ones produced in the full input
inputTemplate: <s> ${delim} Come up with a query optimization plan based on the following SQL query: ${delim} Query: <query> ${delim} Execution Plan: <execPlan>

# determines whether you want to observe inference with random data
# following the above template, or if you want to craft your own prompt
# options: manual|generate
sampleMode: generate

[dataProcessor]
# data processing mode: unused atm
mode: val
dpSource: /data/raw
dpDest: /data/clean
