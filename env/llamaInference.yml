name: llamaInference
channels:
  - defaults
dependencies:
  - bzip2=1.0.8=he774522_0
  - ca-certificates=2023.08.22=haa95532_0
  - libffi=3.4.4=hd77b12b_0
  - openssl=3.0.12=h2bbff1b_0
  - pip=23.3=py311haa95532_0
  - python=3.11.5=he1021f5_0
  - setuptools=68.0.0=py311haa95532_0
  - sqlite=3.41.2=h2bbff1b_0
  - tk=8.6.12=h2bbff1b_0
  - vc=14.2=h21ff451_1
  - vs2015_runtime=14.27.29016=h5e58377_2
  - wheel=0.41.2=py311haa95532_0
  - xz=5.4.2=h8cc25b3_0
  - zlib=1.2.13=h8cc25b3_0
  - pip:
      - aiohttp==3.8.6
      - aiosignal==1.3.1
      - async-timeout==4.0.3
      - attrs==23.1.0
      - certifi==2023.7.22
      - charset-normalizer==3.3.2
      - colorama==0.4.6
      - datasets==2.14.6
      - dill==0.3.7
      - et-xmlfile==1.1.0
      - evaluate==0.4.1
      - filelock==3.13.1
      - frozenlist==1.4.0
      - fsspec==2023.10.0
      - huggingface==0.0.1
      - huggingface-hub==0.17.3
      - idna==3.4
      - jinja2==3.1.2
      - markupsafe==2.1.3
      - mpmath==1.3.0
      - multidict==6.0.4
      - multiprocess==0.70.15
      - networkx==3.2.1
      - numpy==1.26.1
      - openpyxl==3.1.2
      - packaging==23.2
      - pandas==2.1.2
      - pyarrow==14.0.0
      - pyperclip==1.8.2
      - pytz==2023.3.post1
      - pyyaml==6.0.1
      - regex==2023.10.3
      - requests==2.31.0
      - responses==0.18.0
      - safetensors==0.4.0
      - six==1.16.0
      - sympy==1.12
      - tokenizers==0.14.1
      - torch==2.1.0
      - tqdm==4.66.1
      - transformers==4.35.0
      - typing-extensions==4.8.0
      - tzdata==2023.3
      - urllib3==2.0.7
      - xxhash==3.4.1
      - yarl==1.9.2
prefix: C:\Users\mashu\miniconda3\envs\llamaInference
