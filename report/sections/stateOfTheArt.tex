\section{Related Work}

Ali et al. present an approach for optimizing the querying of large-scale heterogeneous models in low-code platforms through compile-time static analysis and specific query optimizers/translators, aiming to improve query execution time and memory footprint \cite{Ali2020EfficientlyQM}. They do not fully address real-time adaptability in highly dynamic or unpredictable data environments, but it may be fruitful to combine their techniques with LLM optimization in the future. Additionally, a well-trained LLM may be able to provide more robust optimization.

Ambite and Knoblock present a novel approach to query planning in mediators using the Planning by Rewriting (PbR) paradigm, focusing on optimizing plan quality through efficient local search techniques and flexible, scalable system design, demonstrating improved scalability and plan quality over existing methods \cite{Ambite2000FlexibleQP}. LLMs can leverage PbR by enabling a systematic rewriting of queries into more efficient forms based on pre-defined optimization rules and the specific structure of the database. We do not specifically apply PbR to our application, but this could be fruitful in future exploration.

Li, Zhou and Cao identify three primary ways artificial intelligence is being used to optimize database performance \cite{li2021}: cost estimation, join order, and complete optimization. Cost estimation plays a major role in join order selection, as performing joins and filters on smaller tables before larger ones decreases search times and memory usage. A query's execution plan can vary in many fundamental and nuanced characteristics, yielding millions of execution options. Thus, the following AI-based algorithms have also been used in this area to quickly arrive at an approximately optimal plan.

Ji Sun and Guoliang Li implemented an advanced cost estimation model \cite{sun2019}. This model consisted of three layers to handle embedding, high-level query representation, and output. In order to capture the tree-like requirements of most queries and their execution plans, the model's structure was tree-based and allowed nodes to learn their subnodes' execution plans. Once this structure was achieved in the representation-layer, the estimation layer was able to produce more accurate costs than several baselines. Nevertheless, this approach suffers from needing to train a database-specific model for each application. Thus, it gains accuracy at the expense of generality.

Wang et al. explore another aspect relating to our research in their development of LANTERN, a tool for explaining SQL query execution plans \cite{wang2021}. While achieving an 86\% success rate in a related task, this study only describes query execution plans rather than generating them. It is useful for human learners, but not for database management systems (DBMS).

In the context of leveraging LLMs for database query optimization, our research uniquely addresses the integration of general-purpose LLMs, like Llama 2, without extensive, database-specific training. While previous works have focused on custom-built models or theoretical frameworks for AI-enhanced database management, our approach seeks to explore and validate the practical applicability of existing LLMs in generating efficient query execution plans. This fills a significant gap by providing insights into the potential of non-specialized LLMs to contribute to database optimization, thereby offering a broader, more accessible foundation for future advancements in AI-driven database technologies.
