{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning and Model Research\n",
    "### Deciding on an Approach\n",
    "Various approaches can be used for transfer learning. One of the first ideas to explore is how transfer learning itself is done. There are papers on this topic, such as In their paper on Transfer Learning [An Embarrassingly Simple Approach for Transfer Learning from Pretrained Language Models](https://doi.org/10.48550/arXiv.1902.10547)  that explain the core of transfer learning and also the parts that you can play around with. Below I explore how transfer learning is actually accomplished.\n",
    "\n",
    "#### Similar Projects\n",
    "One way to get started is to check out other's projects. Here are several that seem quite similar to my goal:\n",
    "* [Simplifying Paragraph-level Question Generation via Transformer Language Models](https://paperswithcode.com/paper/transformer-based-end-to-end-question) uses GPT-2 Small as a base. Then trains on top and produces natural questions. This one is particularly good because it generates **extractive** questions and answers, which is exactly what PBE competitions are all about.\n",
    "* [Learning to Ask: Neural Question Generation for Reading Comprehension](https://paperswithcode.com/paper/learning-to-ask-neural-question-generation) is trained to ask questions. It avoids the rule-based approach and performs better on generating natural and complex questions than other approaches. This seems very promising.\n",
    "* [Neural Question Generation from Text: A Preliminary Study](https://paperswithcode.com/paper/neural-question-generation-from-text-a) uses [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) to generate \"fluent and diverse questions.\"\n",
    "* [Asking Questions the Human Way: Scalable Question-Answer Generation from Text Corpus](https://paperswithcode.com/paper/asking-questions-the-human-way-scalable) presents a seemingly successful question and answer generation project. However, running it looks difficult and there may be \"clues\" required to generate good questions.\n",
    "* [A BERT Baseline for Natural Questions](https://paperswithcode.com/dataset/natural-questions) this paper uses a natural question dataset. Unfortunately, it seems more interested in answering questions rather than asking them.\n",
    "* [ChatDoctor](http://www.yunxiangli.top/ChatDoctor/). This AI was trained on 220K conversations between doctors and patients to learn to converse and follow instructions. It was built on the the back of the Large Language Model Meta AI (LlaMA).\n",
    "* [Generating Natural Questions About an Image](https://paperswithcode.com/paper/generating-natural-questions-about-an-image) is a very cool paper with a cool dataset. However, it is not what I need for text AQG.\n",
    "* [Unified Language Model Pre-training for Natural Language Understanding and Generation](https://paperswithcode.com/paper/unified-language-model-pre-training-for) claims to do question generation with the [SQuAD](https://rajpurkar.github.io/SQuAD-explorer/) database. Doesn't seem quite right though.\n",
    "* \n",
    "\n",
    "#### Promising datasets\n",
    "* [TriviaQA](http://nlp.cs.washington.edu/triviaqa/) has questions, answers, and usually, a context paragraph.\n",
    "* The Stanford Question Answering Dataset ([SQuAD](https://rajpurkar.github.io/SQuAD-explorer/)) contains questions, answers, and usually, a context paragraph. This data could be used in an \"inverted\" way to generate questions.\n",
    "* [harvestingQA](https://github.com/xinyadu/harvestingQA/tree/master/dataset) is a question-answer dataset in SQuAD format.\n",
    "* [SciQ](https://paperswithcode.com/dataset/sciq) also contains questions, answers, and a supporting paragraph.\n",
    "* My own dataset of PBE questions.\n",
    "\n",
    "#### Promising models\n",
    "* [potsawee/t5-large-generation-squad-QuestionAnswer](https://huggingface.co/potsawee/t5-large-generation-squad-QuestionAnswer) generates questions but is not ideal\n",
    "* [patil-suraj/question_generation](https://github.com/patil-suraj/question_generation) seems like a state-of-the-art model, but it will probably require some coercing to get the question and answers out of it.\n",
    "* [abhitopia/question-answer-generation](https://huggingface.co/abhitopia/question-answer-generation) looks like a good model, but requires question generation\n",
    "\n",
    "#### Other things I have checked out\n",
    "* [QuestGen](https://app.questgen.ai/) markets itself as a quiz question generator, but it **does not** generate good questions for PBE. The questions tend to be way too abstract for PBE competitions."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages and Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First I install pyTorch. It's weird that I have to do this to satisfy the jupyter notebook, as the package is already installed for my python environment locally, but this is what it takes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a list of generally useful libraries for my project. I prefer to install them all at once so I can see them all and avoid circular dependencies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "# load transformers\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, T5Config, T5ForConditionalGeneration, T5Tokenizer"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing data\n",
    "Next I will load my input data from `nkjv.json` and store it as a csv for easier access in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open the JSON file and load the data\n",
    "with open('nkjv.json') as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "# open the CSV file for writing\n",
    "with open('nkjv.csv', 'w', newline='') as f:\n",
    "  writer = csv.writer(f)\n",
    "\n",
    "  # Write the header row\n",
    "  writer.writerow(['Book', 'ChapterNumber', 'VerseNumber', 'Verse'])\n",
    "\n",
    "  # Loop through the data and write each row to the CSV file\n",
    "  for book in data:\n",
    "    book_name = book['name']\n",
    "    for chapter_num, chapter in enumerate(book['chapters'], 1):\n",
    "      for verse in chapter['verses']:\n",
    "        verse_num = verse['num']\n",
    "        verse_text = verse['text']\n",
    "        writer.writerow([book_name, chapter_num, verse_num, verse_text])\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now I load the csv and then get started with some question generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nkjv = pd.read_csv('nkjv.csv')\n",
    "\n",
    "joshua = nkjv[nkjv['Book'] == 'Joshua']\n",
    "joshua1 = joshua[joshua['ChapterNumber'] == 1]\n",
    "joshua2 = joshua[joshua['ChapterNumber'] == 2]\n",
    "joshua3 = joshua[joshua['ChapterNumber'] == 3]\n",
    "joshua4 = joshua[joshua['ChapterNumber'] == 4]\n",
    "\n",
    "nkjv.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Transformers and PyTorch with Existing Models\n",
    "I first try out what the paper \"[Simplifying Paragraph-level Question Generation via Transformer Language Models](https://paperswithcode.com/paper/transformer-based-end-to-end-question)\" with it's hugging-face packages as well as the T5 hugging-face packages for [Question Generation](https://huggingface.co/mrm8488/t5-base-finetuned-question-generation-ap) and [Q&A Generation](https://huggingface.co/potsawee/t5-large-generation-squad-QuestionAnswer).\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I first define a few different functions that can generate questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# potsawee_T5 is a model taken from https://huggingface.co/potsawee/t5-large-generation-squad-QuestionAnswer\n",
    "potsawee_tokenizer = AutoTokenizer.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "potsawee_model = AutoModelForSeq2SeqLM.from_pretrained(\"potsawee/t5-large-generation-squad-QuestionAnswer\")\n",
    "\n",
    "def potsawee_T5(text):\n",
    "  inputs = potsawee_tokenizer(text, return_tensors=\"pt\")\n",
    "  outputs = potsawee_model.generate(**inputs, max_length=100)\n",
    "  question_answer = potsawee_tokenizer.decode(outputs[0], skip_special_tokens=False)\n",
    "  question_answer = question_answer.replace(potsawee_tokenizer.pad_token, \"\").replace(potsawee_tokenizer.eos_token, \"\")\n",
    "  return question_answer.split(potsawee_tokenizer.sep_token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# allenai_t5 model from https://huggingface.co/allenai/t5-small-squad2-question-generation\n",
    "model_name = \"allenai/t5-small-squad2-question-generation\"\n",
    "allenai_t5_tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
    "allenai_t5_model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "\n",
    "def allenai_t5(input_string, **generator_args):\n",
    "    input_ids = allenai_t5_tokenizer.encode(input_string, return_tensors=\"pt\")\n",
    "    res = allenai_t5_model.generate(input_ids, **generator_args)\n",
    "    output = allenai_t5_tokenizer.batch_decode(res, skip_special_tokens=True)\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "\n",
    "allenai_t5(\"shrouds herself in white and walks penitentially disguised as brotherly love through factories and parliaments; offers help, but desires power;\")\n",
    "allenai_t5(\"He thanked all fellow bloggers and organizations that showed support.\")\n",
    "allenai_t5(\"Races are held between April and December at the Veliefendi Hippodrome near Bakerky, 15 km (9 miles) west of Istanbul.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through every chapter and verse in Joshua 2\n",
    "print(\"_____________potsawee_T5 model_____________\")\n",
    "for index, row in joshua2.iterrows():\n",
    "  num = row['VerseNumber']\n",
    "  context = row['Verse']\n",
    "  print(f\"Joshua 1:{num}: {context}\")\n",
    "  question, answer = potsawee_T5(context)\n",
    "\n",
    "  print(\"Q:\", question)\n",
    "  print(\"A:\", answer)\n",
    "\n",
    "print(\"_____________allenai_T5 model_____________\")\n",
    "for index, row in joshua2.iterrows():\n",
    "  num = row['VerseNumber']\n",
    "  context = row['Verse']\n",
    "  print(f\"Joshua 1:{num}: {context}\")\n",
    "  questions = allenai_t5(context)\n",
    "  for question in questions:\n",
    "    print(\"Q:\", question)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see above, the questions generated are not ideal. However, no other models seem better. The potsawee_t5 model seems to create more relevant questions than the allenai_t5 and the allenai_t5 does not produce answers, so I will try to work with the potsawee_t5. See [`transferLearningModel.ipynb`](transferLearningModel.ipynb) for the creation of a new model and for results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
